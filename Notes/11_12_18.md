#Recurrent Neural Networks

- Same number of input nodes as the number of features
- Same number of output nodes as the number of classes (in a classification RNN)
- Context layer in RNN keeps track of previous pieces of data
- RNN is commonly used to analyze time series data
- ANN's tend to want their inputs to be between the values of 0 and 1.  So this will require the user to normalize or otherwise massage the data before it gets fed to a neural net


```R
rm(list=ls()); cat("\014") # clear all
library('RSNNS')
data('snnsData')
laser <- snnsData$laser_1000.pat
inputs <- laser[,inputColumns(laser)]
targets <- laser[,outputColumns(laser)]
patterns <- splitForTrainingAndTest(inputs, targets, ratio = 0.15)
model <- elman(patterns$inputsTrain, patterns$targetsTrain, size = c(8, 8), 
               learnFuncParams = c(0.1), maxit= 500, inputsTest = patterns$inputsTest, 
               targetsTest = patterns$targetsTest, linOut = FALSE)

plot(c(0, 100), c(0, 0.9), type = 'n')
lines(inputs[1:100], col = 'black', lwd = 2.5)
lines(targets[1:100], col = 'blue', lwd = 2.5)
lines(model$fittedTestValues, col = 'red', lwd = 2.5)
```

#Web Mining

- Number of pages on the web is "infinitely" large
- Google claims their index contains 1 trillion pages
- The web is a directed graph (once your edges have a direction it is no longer a "graph" but rather a "network")
-- pages are nodes
-- links are edges
- Apparently, the in-degree of nodes on the web follow a power law relationship (very many pages have in-degree of 1, with very few pages have very large in-degree)
-- power law equation is y = x^(-b) = 1 / x^b 

##EDGAR (see slides)

- There is an R library to access EDGAR (edgarWebR)

```R
library('edgarWebR')
ticker <- 'FRO' # company ticker
res <- company_information(ticker)
res2 <- edgarWebR::company_information(ticker)
# knitr is in base R
# no need to use 'library' to bring in the library.  you can use :: to access it
# through it's name
knitr::kable(res[,1:8], digits = 2, format.args = list(big.mark = ','))
knitr::kable(res[,9:16], digits = 2, format.args = list(big.mark = ','))

filings <- company_filings(ticker, count = 100)
initial_count <- nrow(filings)
unique(filings$type)
docs <- filing_documents(filings$href[1])
names(docs)
docs$href
# the below fails for me
doc <- parse_filing(docs$href[2], include.raw = TRUE)
# Error in doc_parse_raw(x, encoding = encoding, base_url = base_url, as_html = as_html,  : 
#   Excessive depth in document: 256 use XML_PARSE_HUGE option [1]
head(doc$text, 10)
```


##Web Scraping

```R
library('rvest')
movies <- read_html('http://www.coolidge.org/showtimes/')
titles <- html_nodes(movies, '#view-id-external_showtime_date_browser-page div.film-event-title')
html_text(titles)
```


##Web sources analysis in R
- Use the tm.plugin.webmining package - an open source framework for web mining applications
- Retrieve textual data using the WebSoures (analagous to "Source" in tm)
-- extracts the metadata from the source
- WebCorpus (rather than "Corpus" in regular tm) to create a corpus
-- extracts the textual data from the source

#Shiny

